{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Keras Deep Neural Net for emotions detection\nIn this project we will use Kaggle's [Face expression recognition dataset](https://www.kaggle.com/jonathanoheix/face-expression-recognition-dataset). It has images of faces with 7 different expressions: angry, disgust, fear, neutral, happy, sad, surprise. In Emo Rec app we will need only three expressions: happy, sad and neutral.","metadata":{"_uuid":"b2676e8f24dcdede793d3efd7cfb84f6c624f695"}},{"cell_type":"markdown","source":"# Data visualization","metadata":{}},{"cell_type":"code","source":"# display some images for every different expression\n\nimport numpy as np\nimport seaborn as sns\nfrom keras.preprocessing.image import load_img, img_to_array\nimport matplotlib.pyplot as plt\nimport os\n\n# size of the image: 48*48 pixels\npic_size = 48\n\n# input path for the images\nbase_path = \"../input/images/images/\"\n\nplt.figure(0, figsize=(12,20))\ncpt = 0\n\nfor expression in os.listdir(base_path + \"train/\"):\n    for i in range(1,6):\n        cpt = cpt + 1\n        plt.subplot(7,5,cpt)\n        img = load_img(base_path + \"train/\" + expression + \"/\" +os.listdir(base_path + \"train/\" + expression)[i], target_size=(pic_size, pic_size))\n        plt.imshow(img, cmap=\"gray\")\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"ada86668a25cba36e2676a20bc76f96581057d26","execution":{"iopub.status.busy":"2022-01-12T15:23:50.845025Z","iopub.execute_input":"2022-01-12T15:23:50.845807Z","iopub.status.idle":"2022-01-12T15:23:57.331616Z","shell.execute_reply.started":"2022-01-12T15:23:50.845747Z","shell.execute_reply":"2022-01-12T15:23:57.330966Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Setup the data generators","metadata":{"_uuid":"74f292f501e5a60adb5463f412f42e5584f3a6a5","trusted":true}},{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\n# number of images to feed into the NN for every batch\nbatch_size = 128\n\ndatagen_train = ImageDataGenerator()\ndatagen_validation = ImageDataGenerator()\n\n# we use only three classes\nclasses = [\"sad\", \"happy\", \"neutral\"]\n\ntrain_generator = datagen_train.flow_from_directory(base_path + \"train\",\n                                                    target_size=(pic_size,pic_size),\n                                                    color_mode=\"grayscale\",\n                                                    batch_size=batch_size,\n                                                    class_mode='categorical',\n                                                    shuffle=True,\n                                                    classes=classes)\n\nvalidation_generator = datagen_validation.flow_from_directory(base_path + \"validation\",\n                                                    target_size=(pic_size,pic_size),\n                                                    color_mode=\"grayscale\",\n                                                    batch_size=batch_size,\n                                                    class_mode='categorical',\n                                                    shuffle=False,\n                                                    classes=classes)","metadata":{"_uuid":"0041128a27f4da936ad63e90959797391736fc8b","execution":{"iopub.status.busy":"2022-01-12T15:23:57.333303Z","iopub.execute_input":"2022-01-12T15:23:57.333724Z","iopub.status.idle":"2022-01-12T15:24:11.123913Z","shell.execute_reply.started":"2022-01-12T15:23:57.333550Z","shell.execute_reply":"2022-01-12T15:24:11.123112Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Convolutional Neural Network\nWe will build CNN with Keras' Sequential API. Our network will have 4 convolutional and 2 fully-connected layers. And we will use Adam optimizer:","metadata":{}},{"cell_type":"code","source":"from keras.layers import Dense, Input, Dropout, GlobalAveragePooling2D, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam\n\n# number of possible label values\nnb_classes = len(classes)\n\n# Initialising the CNN\nmodel = Sequential()\n\n# 1 - Convolution\nmodel.add(Conv2D(64,(3,3), padding='same', input_shape=(48, 48,1)))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 2nd Convolution layer\nmodel.add(Conv2D(128,(5,5), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 3rd Convolution layer\nmodel.add(Conv2D(512,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# 4th Convolution layer\nmodel.add(Conv2D(512,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# Flattening\nmodel.add(Flatten())\n\n# Fully connected layer 1st layer\nmodel.add(Dense(256))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\n# Fully connected layer 2nd layer\nmodel.add(Dense(512))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(nb_classes, activation='softmax'))\n\nopt = Adam(lr=0.0001)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"_uuid":"4914afe556e0db3c03614ed1ac2138fd68d03815","execution":{"iopub.status.busy":"2022-01-12T15:24:11.124948Z","iopub.execute_input":"2022-01-12T15:24:11.125220Z","iopub.status.idle":"2022-01-12T15:24:13.222447Z","shell.execute_reply.started":"2022-01-12T15:24:11.125164Z","shell.execute_reply":"2022-01-12T15:24:13.221634Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Train the model","metadata":{"_uuid":"883ca4ab4d9c00ac08434916e7713c8f9b4353ba","trusted":true}},{"cell_type":"code","source":"# number of epochs to train the NN\nepochs = 25\n\nhistory = model.fit_generator(generator=train_generator,\n                                steps_per_epoch=train_generator.n//train_generator.batch_size,\n                                epochs=epochs,\n                                validation_data = validation_generator,\n                                validation_steps = validation_generator.n//validation_generator.batch_size,)","metadata":{"_uuid":"8d3db222ae6b74ab55df0d840d372b4306af6db0","scrolled":true,"execution":{"iopub.status.busy":"2022-01-12T15:24:32.967869Z","iopub.execute_input":"2022-01-12T15:24:32.968152Z","iopub.status.idle":"2022-01-12T15:31:15.585956Z","shell.execute_reply.started":"2022-01-12T15:24:32.968100Z","shell.execute_reply":"2022-01-12T15:31:15.584127Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Our model has a validation accuracy of approximately 73%.\n\nWe also have to save our model into a file to use it in our web app:","metadata":{"_uuid":"2e9eadc450fe57d829f8acb4d40a810b9367c08b"}},{"cell_type":"code","source":"# saving model\nmodel.save(\"model\")","metadata":{"execution":{"iopub.status.busy":"2022-01-12T15:31:54.496329Z","iopub.execute_input":"2022-01-12T15:31:54.496614Z","iopub.status.idle":"2022-01-12T15:31:54.961370Z","shell.execute_reply.started":"2022-01-12T15:31:54.496564Z","shell.execute_reply":"2022-01-12T15:31:54.960669Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Confusion matrix","metadata":{"_uuid":"25e7825e2fd5cbff9cac17a577ff2925d476584d"}},{"cell_type":"code","source":"# compute predictions\npredictions = model.predict_generator(generator=validation_generator)\ny_pred = [np.argmax(probas) for probas in predictions]\ny_test = validation_generator.classes\nclass_names = validation_generator.class_indices.keys()\n\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    plt.figure(figsize=(10,10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    \n# compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)\n\n# plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names, title='Normalized confusion matrix')\nplt.show()","metadata":{"_uuid":"4d2a670697c28add1f73f5d41c129c2df974f7b0","execution":{"iopub.status.busy":"2022-01-12T15:32:01.736878Z","iopub.execute_input":"2022-01-12T15:32:01.737159Z","iopub.status.idle":"2022-01-12T15:32:05.233543Z","shell.execute_reply.started":"2022-01-12T15:32:01.737110Z","shell.execute_reply":"2022-01-12T15:32:05.232690Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Our model is very good for predicting happy faces. However it predicts quite poorly feared sad because it confuses them with neutral faces.","metadata":{"_uuid":"f6f66768b63e000a9dcdcbd64d30d5f66ef55a37","trusted":true}}]}